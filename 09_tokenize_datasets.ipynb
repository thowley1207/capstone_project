{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMx+Wc5oDnjk64lHZ60M3va",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thowley1207/capstone_project/blob/main/09_tokenize_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tKDKZabvf4G"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import (Dataset, DatasetDict, ClassLabel,\n",
        "                      concatenate_datasets, load_dataset)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a helper functions for use in tokenizing the cleaned and preprocessed 8K text entries in the dataset_stacked_{2/3}_labels dataframes created in 08_create_and_push_datasets:**\n",
        "\n",
        "        def shape_tokenize_function(example):\n",
        "\n",
        "        def base_tokenize_function(examples):\n",
        "\n",
        "* **Functions input parameter is:**\n",
        "         # The desired stacked dataframe containing event_id, labels, text, and\n",
        "         #   the additional descriptor details added last script\n",
        "         examples\n",
        "\n",
        "* **Function returns:**\n",
        "        # The AutoTokenizer.from_pretrained({relevant_model_loc}) object that\n",
        "        #     the name of the function indicates (sec-bert-shape for the #.\n",
        "        #     shape_tokenize_function, sec-bert-base for the\n",
        "        #     base_tokenize_function)\n",
        "        base_tokenizer / shape_tokenizer object\n",
        "\n",
        "These functions are soon aplied to the stacked datasets via the dataset map functionality to efficiently create the tokenized data for both versions of the text input included as columns\n",
        "\n"
      ],
      "metadata": {
        "id": "094tPhoKnspG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-shape\")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-base\")\n",
        "\n",
        "def shape_tokenize_function(example):\n",
        "    return shape_tokenizer(example[\"text_8k_sec_bert_shape\"],\n",
        "                           padding = \"max_length\",\n",
        "                           truncation=True)\n",
        "\n",
        "\n",
        "def base_tokenize_function(examples):\n",
        "    return base_tokenizer(examples['text_8k_sec_bert_base'],\n",
        "                          padding = 'max_length',\n",
        "                          truncation=True)"
      ],
      "metadata": {
        "id": "YHeotVK3yl3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create several datasets from subsets of the overall 2 label and 3 label stacked datasets with different characteristics.**\n",
        "\n",
        "**NOTE:** for each subset we reference in the descriptions provided below, the subset should be assumed to have been generated for both the 2 label and the 3 label data\n",
        "* This is assumed because it is impossible not to consider this data seperately, as the labels cause each of the respective datasets to have a fundamental difference in the definition of each's class label.\n",
        "\n",
        "\n",
        "We will fine tune several of the eventual final resultant datasets that are created in this process.\n",
        "\n",
        "* They will allow us to observe whether there are any obvious patterns or differences between fine tuning performance based on the contrasting characteristics of the datasets (or conversely if the result is inconclusive).\n",
        "\n",
        "* Datasets with different combinations of the following differing characteristics will be generated in this process and pushed to the hub:\n",
        "\n",
        "    * **Text tokenized using sec-bert-base vs. text tokenized using sec-bert-shape** (these cannot remain in the same Dataset because the model accepts specifically formatted Datasets as training and evaluation data)\n",
        "    * Data with **labels representing 2 bins** vs. data with **labels representing 3 bins**\n",
        "    * Labels based on the **non-standardized CAR values** associated with events vs. labels based on the **standardized CAR values** associated with the events**\n",
        "    * Labels generated from the results of event studies with a **long event window (starting 5 days before the event date, ending 5 days after)** vs. labels generated from the results of event studies with a **short event window (starting 1 day before the event, ending 1 day after)**\n",
        "        * **NOTE:** in the prior work, we have generated event study results based on many different event windows.\n",
        "            * However, due to time constraints, rather than testing all possible permutations (of which there are 72) we will only fine-tune the data corresponding to the longest and shortest symetric windows we utilized"
      ],
      "metadata": {
        "id": "2fqE_alNWCuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2_labels = load_dataset(\"thowley824/dataset_stacked_2_labels\")\n",
        "dataset_3_labels = load_dataset(\"thowley824/dataset_stacked_3_labels\")"
      ],
      "metadata": {
        "id": "VWcmpR1JxET8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Convert the text columns to tokenized datasets using the tokenizer functions above**\n",
        "\n",
        "* To do so, pass the tokenizer function to the dataset map function as an argument."
      ],
      "metadata": {
        "id": "e5eccI5pVWLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_2_labels = dataset_2_labels.map(\n",
        "    shape_tokenize_function, batched=True)\n",
        "\n",
        "shape_3_labels = dataset_3_labels.map(\n",
        "    shape_tokenize_function, batched=True)\n",
        "\n",
        "base_2_labels = dataset_2_labels.map(\n",
        "    base_tokenize_function, batched=True)\n",
        "\n",
        "base_3_labels = dataset_3_labels.map(\n",
        "    base_tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "j2uHZ4IAVr0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create long and short window datasets from the tokenized datasets**\n",
        "\n",
        "* To do so, pass a lambda function to the Dataset filter function resulting in the retention of only data with the desired event window start and ends."
      ],
      "metadata": {
        "id": "JVUpF5zav_6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_long_window_2_labels = shape_2_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n",
        "shape_long_window_3_labels = shape_3_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n",
        "\n",
        "shape_short_window_2_labels = shape_2_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))\n",
        "shape_short_window_3_labels = shape_3_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))\n",
        "\n",
        "base_long_window_2_labels = base_2_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n",
        "base_long_window_3_labels = base_3_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-5)&(x['event_window_end']==5))\n",
        "\n",
        "base_short_window_2_labels = base_2_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))\n",
        "base_short_window_3_labels = base_3_labels.filter(\n",
        "    lambda x: (x['event_window_start']==-1)&(x['event_window_end']==1))"
      ],
      "metadata": {
        "id": "zLUDOzcH28p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Create CAR-label-based and SCAR-label-based datasets from the long and short window / base and shape tokenized data created in Step 2.**"
      ],
      "metadata": {
        "id": "m4ye_aF3x1-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_long_window_2_labels_car = shape_long_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "shape_long_window_3_labels_car = shape_long_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "shape_long_window_2_labels_scar = shape_long_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "shape_long_window_3_labels_scar = shape_long_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "base_long_window_2_labels_car = base_long_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "base_long_window_2_labels_scar = base_long_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "base_long_window_3_labels_car = base_long_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "base_long_window_3_labels_scar = base_long_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "shape_short_window_2_labels_car = shape_short_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "shape_short_window_2_labels_scar = shape_short_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "shape_short_window_3_labels_car = shape_short_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "shape_short_window_3_labels_scar = shape_short_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "base_short_window_2_labels_car = base_short_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "base_short_window_2_labels_scar = base_short_window_2_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))\n",
        "\n",
        "base_short_window_3_labels_car = base_short_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='car'))\n",
        "\n",
        "base_short_window_3_labels_scar = base_short_window_3_labels.filter(\n",
        "    lambda x: (x['abnormal_return_metric']=='scar'))"
      ],
      "metadata": {
        "id": "krWAwPIiT1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Clean up the dataset formatting for each of the created datasets so that they can be used in model fine-tuning and testing.**\n",
        "\n",
        "- Remove all columns from every dataset created in Step 3 except for those created by the tokenizer and the label column.\n",
        "\n",
        "- Rename the label column labels to conform with model input requirements."
      ],
      "metadata": {
        "id": "CfDbpL_NUdSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_columns = [\n",
        "    'event_id','text_8k_sec_bert_base','text_8k_sec_bert_shape',\n",
        "    'event_window_start','event_window_end','abnormal_return_metric']\n",
        "\n",
        "\n",
        "shape_long_window_2_labels_car = shape_long_window_2_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_long_window_2_labels_car = shape_long_window_2_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_long_window_3_labels_car = shape_long_window_3_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_long_window_3_labels_car = shape_long_window_3_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_long_window_2_labels_scar = shape_long_window_2_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_long_window_2_labels_scar = shape_long_window_2_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_long_window_3_labels_scar = shape_long_window_3_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_long_window_3_labels_scar = shape_long_window_3_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_long_window_2_labels_car = base_long_window_2_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_long_window_2_labels_car = base_long_window_2_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_long_window_2_labels_scar = base_long_window_2_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_long_window_2_labels_scar = base_long_window_2_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_long_window_3_labels_car = base_long_window_3_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_long_window_3_labels_car = base_long_window_3_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_long_window_3_labels_scar = base_long_window_3_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_long_window_3_labels_scar = base_long_window_3_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_short_window_2_labels_car = shape_short_window_2_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_short_window_2_labels_car = shape_short_window_2_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_short_window_2_labels_scar = shape_short_window_2_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_short_window_2_labels_scar = shape_short_window_2_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_short_window_3_labels_car = shape_short_window_3_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_short_window_3_labels_car = shape_short_window_3_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "shape_short_window_3_labels_scar = shape_short_window_3_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "shape_short_window_3_labels_scar = shape_short_window_3_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_short_window_2_labels_car = base_short_window_2_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_short_window_2_labels_car = base_short_window_2_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "\n",
        "base_short_window_2_labels_scar = base_short_window_2_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_short_window_2_labels_scar = base_short_window_2_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "base_short_window_3_labels_car = base_short_window_3_labels_car.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_short_window_3_labels_car = base_short_window_3_labels_car.rename_column(\n",
        "    \"label\", \"labels\")\n",
        "\n",
        "base_short_window_3_labels_scar = base_short_window_3_labels_scar.map(\n",
        "    remove_columns = remove_columns)\n",
        "base_short_window_3_labels_scar = base_short_window_3_labels_scar.rename_column(\n",
        "    \"label\", \"labels\")"
      ],
      "metadata": {
        "id": "mFuXuCYdBwDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Push the final version of the datasets to the Hugging Face Hub.**"
      ],
      "metadata": {
        "id": "KPVihpNsVFsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape_long_window_2_labels_car.push_to_hub('shape_long_window_2_labels_car')\n",
        "shape_long_window_3_labels_car.push_to_hub('shape_long_window_3_labels_car')\n",
        "base_long_window_2_labels_car.push_to_hub('base_long_window_2_labels_car')\n",
        "base_long_window_3_labels_car.push_to_hub('base_long_window_3_labels_car')\n",
        "shape_short_window_2_labels_car.push_to_hub('shape_short_window_2_labels_car')\n",
        "shape_short_window_3_labels_car.push_to_hub('shape_short_window_3_labels_car')\n",
        "base_short_window_2_labels_car.push_to_hub('base_short_window_2_labels_car')\n",
        "base_short_window_3_labels_car.push_to_hub('base_short_window_3_labels_car')\n",
        "\n",
        "shape_long_window_2_labels_scar.push_to_hub('shape_long_window_2_labels_scar')\n",
        "shape_long_window_3_labels_scar.push_to_hub('shape_long_window_3_labels_scar')\n",
        "base_long_window_2_labels_scar.push_to_hub('base_long_window_2_labels_scar')\n",
        "base_long_window_3_labels_scar.push_to_hub('base_long_window_3_labels_scar')\n",
        "shape_short_window_2_labels_scar.push_to_hub('shape_short_window_2_labels_scar')\n",
        "shape_short_window_3_labels_scar.push_to_hub('shape_short_window_3_labels_scar')\n",
        "base_short_window_2_labels_scar.push_to_hub('base_short_window_2_labels_scar')\n",
        "base_short_window_3_labels_scar.push_to_hub('base_short_window_3_labels_scar')"
      ],
      "metadata": {
        "id": "y1xYwfh6dLPQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}