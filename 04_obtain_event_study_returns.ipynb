{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPeSkX8j4NZAPJ9rANNo+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thowley1207/capstone_project/blob/04/04_obtain_event_study_returns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wrds\n",
        "!wget https://raw.githubusercontent.com/thowley1207/capstone_project/main/colab_initialization/initializer.py\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import initializer\n",
        "initializer.initialize_colab()\n",
        "db = initializer.initialize_wrds_connection()"
      ],
      "metadata": {
        "id": "TVLWolvygEmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz8ZHdcsf5QB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "SET DATA SUBDIRECTORIES AND FORM TYPE PREFIX\n",
        "WHEN APPLICABLE, THIS FORM TYPE PREFIX WILL BE USED MOVING FORWARD\n",
        "'''\n",
        "\n",
        "linking_data_subdir = 'data/edgar_wrds_linking/'\n",
        "event_study_data_subdir = 'data/event_study/returns/'\n",
        "file_prefix = '8k_'\n",
        "\n",
        "'''\n",
        "ADDITIONAL FILE NAMES CARRIED DOWN FROM PRIOR WORK\n",
        "'''\n",
        "\n",
        "event_subset_file_name = 'event_subset.pkl'\n",
        "\n",
        "'''\n",
        "NEW FILE NAMES FOR USE BELOW\n",
        "'''\n",
        "\n",
        "event_study_ret_data_file_names = [\n",
        "    'event_study_ret_data_pt_1.pkl',\n",
        "    'event_study_ret_data_pt_2.pkl',\n",
        "    'event_study_ret_data_pt_3.pkl',\n",
        "    'event_study_ret_data_pt_4.pkl']\n",
        "\n",
        "est_win_data_file_names = [\n",
        "    'est_win_data_pt_1.pkl',\n",
        "    'est_win_data_pt_2.pkl',\n",
        "    'est_win_data_pt_3.pkl',\n",
        "    'est_win_data_pt_4.pkl']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "READ IN EVENT SUBSET DATA\n",
        "'''\n",
        "\n",
        "event_subset = pd.read_pickle((\n",
        "    linking_data_subdir +\n",
        "    file_prefix +\n",
        "    event_subset_file_name\n",
        "    ))"
      ],
      "metadata": {
        "id": "c5v7cvLoLG5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **QUERY HELPER FUNCTION**"
      ],
      "metadata": {
        "id": "CJkleELHhF8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_wrds_query(query_content,\n",
        "                       wrds_session = db,\n",
        "                       output_directory = 'data/',\n",
        "                       output_file = None):\n",
        "\n",
        "    query_result = db.raw_sql(query_content)\n",
        "\n",
        "    if output_file is not None:\n",
        "        output_path = f\"\"\"{output_directory}{output_file}\"\"\"\n",
        "        print(f\"\"\"Writing query result to: {output_path}\"\"\")\n",
        "\n",
        "        if output_file.endswith('.csv'):\n",
        "            query_result.to_csv(output_path)\n",
        "        elif output_file.endswith('.pkl'):\n",
        "            query_result.to_pickle(output_path)\n",
        "        else:\n",
        "            raise Exception(\"Invalid File Format Provided For Output\")\n",
        "        print(f\"\"\"Query result successfully written.\"\"\")\n",
        "    else:\n",
        "        print(f\"\"\"Warning: output is not saved to Google Drive.\"\"\")\n",
        "\n",
        "    return query_result"
      ],
      "metadata": {
        "id": "Hq1Xk6TmhFK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Step 1:**\n",
        "\n",
        "* **Get Event Study Return Data For All Dates**\n",
        "    * For each event, get the return data for all dates beginning with the start of the estimation window and ending with the end of the possible event windows\n",
        "    * Due to the size of this data, we split this output into four parts\n",
        "    * We then obtain the data for each part and write the output seperately for each"
      ],
      "metadata": {
        "id": "psoVxfzfh5Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "period_1 = [200501,200502,200503,200504,200601,200602,200603,\n",
        "            200604,200701,200702,200703,200704,200801,200802]\n",
        "\n",
        "period_2 = [200803,200804,200901,200902,200903,200904,201001,\n",
        "            201002,201003,201004,201101,201102,201103,201104]\n",
        "\n",
        "period_3 = [201201,201202,201203,201204,201301,201302,201303,\n",
        "            201304,201401,201402,201403,201404,201501,201502]\n",
        "\n",
        "period_4 = [201503,201504,201601,201602,201603,201604,201701,\n",
        "            201702,201703,201704,201801,201802,201803,201804]\n",
        "\n",
        "periods_lst = [period_1,period_2,period_3,period_4]"
      ],
      "metadata": {
        "id": "s2BVoC_7hDM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "\n",
        "    l_event_study_ret_data = []\n",
        "    periods = periods_lst[i]\n",
        "\n",
        "    for per in periods:\n",
        "\n",
        "        mod_types_event_subset = event_subset[\n",
        "            event_subset['period'] == per].copy()\n",
        "\n",
        "        for col in mod_types_event_subset.columns:\n",
        "            if mod_types_event_subset[col].dtype == 'datetime64[ns]':\n",
        "                mod_types_event_subset[\n",
        "                    col] = mod_types_event_subset[col].astype(str)\n",
        "\n",
        "        json_event_subset = mod_types_event_subset.to_json(orient=\"records\")\n",
        "\n",
        "        q_event_study = f\"\"\"\n",
        "    with\n",
        "    event_info as\n",
        "    (\n",
        "        select\n",
        "           e.event_id,\n",
        "           e.period,\n",
        "           e.permno,\n",
        "           e.event_date,\n",
        "           e.est_per_start,\n",
        "           e.est_per_end,\n",
        "           e.event_wind_start,\n",
        "           e.event_wind_end\n",
        "       from json_to_recordset('{json_event_subset}') as e(\n",
        "                event_id int,\n",
        "                period int,\n",
        "                permno float,\n",
        "                event_date date,\n",
        "                est_per_start date,\n",
        "                est_per_end date,\n",
        "                event_wind_start date,\n",
        "                event_wind_end date)\n",
        "    ),\n",
        "\n",
        "    crsp_daily as\n",
        "    (\n",
        "        select\n",
        "            dsf.permno,\n",
        "            dsf.date as ret_date,\n",
        "            dsf.ret as sec_return\n",
        "        from crsp_a_stock.dsf as dsf\n",
        "        where dsf.ret is not null\n",
        "        and dsf.date between\n",
        "            (select min(e.est_per_start) from event_info e) and\n",
        "            (select max(e.event_wind_end) from event_info e)\n",
        "        and dsf.permno in (select distinct e.permno from event_info e)\n",
        "    ),\n",
        "\n",
        "    market_returns as\n",
        "    (\n",
        "        select\n",
        "            ff.date as mkt_date,\n",
        "            ff.mktrf as mkt_excess_return,\n",
        "            ff.rf as risk_free,\n",
        "            ff.mktrf + ff.rf as mkt_return\n",
        "        from ff_all.factors_daily as ff\n",
        "        where date between\n",
        "            (select min(e.est_per_start) from event_info e) and\n",
        "            (select max(e.event_wind_end) from event_info e)\n",
        "    ),\n",
        "\n",
        "    crsp_w_market as\n",
        "    (\n",
        "        select\n",
        "            crsp.permno,\n",
        "            crsp.ret_date,\n",
        "            crsp.sec_return,\n",
        "            crsp.sec_return - m.mkt_return as sec_excess_return,\n",
        "            m.mkt_excess_return,\n",
        "            m.risk_free,\n",
        "            m.mkt_return\n",
        "        from crsp_daily crsp\n",
        "            join market_returns m\n",
        "                on crsp.ret_date = m.mkt_date\n",
        "    )\n",
        "\n",
        "    select\n",
        "        e.event_id,\n",
        "        e.period,\n",
        "        e.permno,\n",
        "        e.event_date,\n",
        "        e.est_per_start,\n",
        "        e.est_per_end,\n",
        "        e.event_wind_start,\n",
        "        e.event_wind_end,\n",
        "        cm.ret_date,\n",
        "        cm.sec_return,\n",
        "        cm.sec_excess_return,\n",
        "        cm.mkt_excess_return,\n",
        "        cm.risk_free,\n",
        "        cm.mkt_return,\n",
        "        case\n",
        "            when cm.ret_date between e.est_per_start and e.est_per_end\n",
        "                then 1\n",
        "            else 0\n",
        "        end as est_per_flag,\n",
        "        case\n",
        "            when cm.ret_date\n",
        "                between e.event_wind_start and e.event_wind_end then 1\n",
        "            else 0\n",
        "        end as event_wind_flag,\n",
        "        case\n",
        "            when cm.ret_date = e.event_date then 1\n",
        "            else 0\n",
        "            end as event_date_flag\n",
        "    from event_info e\n",
        "        join crsp_w_market cm\n",
        "            on e.permno = cm.permno\n",
        "            and cm.ret_date between e.est_per_start and e.event_wind_end\n",
        "    order by e.event_id, cm.ret_date;\n",
        "    \"\"\"\n",
        "\n",
        "        event_study_ret_data_per = execute_wrds_query(q_event_study)\n",
        "        print(f'''{per} event study data retreived.\n",
        "        Adding to event study data list.''')\n",
        "\n",
        "        l_event_study_ret_data.append(event_study_ret_data_per)\n",
        "\n",
        "        print(f'''{per} event study data added to combined data list.''')\n",
        "\n",
        "    event_study_ret_data = pd.concat(l_event_study_ret_data,\n",
        "                                     ignore_index = True\n",
        "                                     ).sort_values(by = ['period',\n",
        "                                                         'event_id',\n",
        "                                                         'ret_date'])\n",
        "\n",
        "    date_cols = ['event_date',\n",
        "                 'est_per_start',\n",
        "                 'est_per_end',\n",
        "                 'event_wind_start',\n",
        "                 'event_wind_end',\n",
        "                 'ret_date']\n",
        "\n",
        "    for col in date_cols:\n",
        "        event_study_ret_data[col] = pd.to_datetime(event_study_ret_data[col])\n",
        "\n",
        "    event_study_ret_data_file_name = event_study_ret_data_file_names[i]\n",
        "\n",
        "    event_study_ret_data.to_pickle((\n",
        "        event_study_data_subdir +\n",
        "        file_prefix +\n",
        "        event_study_ret_data_file_names[i]\n",
        "        ))\n",
        "\n",
        "    print(f'''Event study return data part {i+1} generated.\n",
        "    Output written as {event_study_ret_data_file_names[i]}''')"
      ],
      "metadata": {
        "id": "g3Ogczv_i1vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Step 2:**\n",
        "\n",
        "* **Create Event Study Return Data For The Estimation Window Only**\n",
        "    * Although the estimation window data output is smaller (3.6 MB),\n",
        "   we still keep it partioned and write out in pieces\n",
        "    * This is because the size of the event study return data files makes\n",
        "     the possibility of failure higher when reading each\n",
        "     in on a loop, and each operation takes a long time\n",
        "    * This way, if any individual step fails, we will still\n",
        "     have the prior parts saved.\n",
        "    * Additionally, the next step (generating regression parameters) is very time consuming.\n",
        "    * By keeping the estimation window data split in parts,\n",
        "     we are able to monitor the progress of the regression\n",
        "     parameter generation, and if there is issue, we can\n",
        "     adjust our approach and save each regression parameter part individually"
      ],
      "metadata": {
        "id": "INCaIBX5i5g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    event_study_ret_data = pd.read_pickle((\n",
        "        event_study_data_subdir +\n",
        "        file_prefix +\n",
        "        event_study_ret_data_file_names[i]))\n",
        "\n",
        "    est_win_data = event_study_ret_data[event_study_ret_data[\n",
        "        'est_per_flag']==1].drop(columns = [\n",
        "            col for col in event_study_ret_data.columns\n",
        "            if col not in ['event_id','sec_return','mkt_return']])\n",
        "\n",
        "    est_win_data.to_pickle((\n",
        "        event_study_data_subdir +\n",
        "        file_prefix +\n",
        "        est_win_data_file_names[i]))\n",
        "\n",
        "    print(f'''Estimation window data part {i+1} generated.\n",
        "    Output written as {est_win_data_file_names[i]}''')"
      ],
      "metadata": {
        "id": "hNhoGJC2i6D5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
