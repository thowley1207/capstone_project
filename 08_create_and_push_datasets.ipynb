{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO5z4Ho8SsrTJQ/IAESFAI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thowley1207/capstone_project/blob/main/08_create_and_push_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmgmftpqkymo"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/thowley1207/capstone_project/main/colab_initialization/initializer.py\n",
        "!pip install --no-dependencies wrds\n",
        "!pip install datasets\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "from datasets import (Dataset, DatasetDict, ClassLabel,\n",
        "                      concatenate_datasets, load_dataset)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import initializer\n",
        "initializer.initialize_colab()\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "SET PROCESSED DATA SUBDIRECTORIES AND FORM TYPE PREFIX\n",
        "WHEN APPLICABLE, THIS FORM TYPE PREFIX WILL BE USED MOVING FORWARD\n",
        "'''\n",
        "labels_data_subdir = 'data/event_study/labels/'\n",
        "sec_edgar_data_subdir = 'data/sec_edgar/'\n",
        "dataset_inputs_subdir = 'data/dataset_inputs/'\n",
        "\n",
        "dataset_inputs_2_labels_subdir = 'data/dataset_inputs/text_w_2_labels/'\n",
        "dataset_inputs_3_labels_subdir = 'data/dataset_inputs/text_w_3_labels/'\n",
        "\n",
        "file_prefix = '8k_'\n",
        "\n",
        "'''\n",
        "FILE NAMES CARRIED DOWN FROM PRIOR WORK\n",
        "'''\n",
        "\n",
        "labels_2_bins_file_name = 'event_car_data_2_bins.pkl'\n",
        "labels_3_bins_file_name = 'event_car_data_3_bins.pkl'\n",
        "text_cleaned_file_name = 'text_cleaned.pkl'\n",
        "\n",
        "'''\n",
        "NEW FILE NAMES FOR USE BELOW\n",
        "'''\n",
        "text_w_labels_wide_file_name = 'text_w_labels_wide.pkl'"
      ],
      "metadata": {
        "id": "FWz-0eJ6lCfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "READ IN LABELS DATA\n",
        "'''\n",
        "\n",
        "labels_2_bins = pd.read_pickle(\n",
        "    labels_data_subdir +\n",
        "    file_prefix +\n",
        "    labels_2_bins_file_name)\n",
        "\n",
        "labels_3_bins = pd.read_pickle(\n",
        "    labels_data_subdir +\n",
        "    file_prefix +\n",
        "    labels_3_bins_file_name)\n",
        "\n",
        "'''\n",
        "READ IN CLEANED TEXT DATA\n",
        "'''\n",
        "\n",
        "text_cleaned = pd.read_pickle(\n",
        "    sec_edgar_data_subdir +\n",
        "    file_prefix +\n",
        "    text_cleaned_file_name)"
      ],
      "metadata": {
        "id": "lzC5td_NlEqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "LOAD THE sec-bert-shape MODEL TOKENIZER AS WELL AS\n",
        "    THE spacy TOKENIZER\n",
        "REQUIRED TO PREPROCESS MODEL INPUT TEXT WHEN USING\n",
        "SEC BERT SHAPE\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/sec-bert-shape\")\n",
        "spacy_tokenizer = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Qd1nZB6tCinX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HELPER FUNCTION: for use in preprocessing sentences that will be tokenized using the SEC BERT SHAPE pretrained tokenizer.**\n",
        "\n",
        "* This is necessary because the SEC BERT SHAPE tokenizer intentionally replaces numbers with pseudo-tokens that represent the numberâ€™s shape, so numeric expressions (of known shapes) are no longer fragmented.\n",
        "    * Example: '53.2' becomes '[XX.X]'\n",
        "\n",
        "* **NOTE: Code leverages the text preprocessing example function provided on the Hugging Face model document page (https://huggingface.co/nlpaueb/sec-bert-shape)**\n",
        "\n",
        "        sec_bert_shape_preprocess(dset)\n",
        "\n",
        "* **Function input parameters are:**\n",
        "        # The dataset containing the text that requires preprocessing\n",
        "        dset\n",
        "\n",
        "* **Function returns:**\n",
        "        # The input dataset with a new column 'text_8k_sec_bert_base' containing the preprocessed text now included\n",
        "        dset\n",
        "        "
      ],
      "metadata": {
        "id": "D5cizsZVTSg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sec_bert_shape_preprocess(dset):\n",
        "\n",
        "    tokens = [t.text for t in spacy_tokenizer(dset['text_8k_sec_bert_base'])]\n",
        "\n",
        "    processed_text = []\n",
        "    for token in tokens:\n",
        "        if re.fullmatch(r\"(\\d+[\\d,.]*)|([,.]\\d+)\", token):\n",
        "            shape = '[' + re.sub(r'\\d', 'X', token) + ']'\n",
        "\n",
        "            if shape in tokenizer.additional_special_tokens:\n",
        "                processed_text.append(shape)\n",
        "            else:\n",
        "                processed_text.append('[NUM]')\n",
        "\n",
        "        else:\n",
        "            processed_text.append(token)\n",
        "\n",
        "    dset['text_8k_sec_bert_shape'] = ' '.join(processed_text)\n",
        "\n",
        "    return dset"
      ],
      "metadata": {
        "id": "-7Grb8miq8HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Create a dataset containing a wide version of the sec text-event abnormal return label data corresponding to each event.**\n",
        "\n",
        " *\n",
        "**NOTE: This wide-style dataset is meant to be a visual representation of the data for each event.**\n",
        "* While it can be used as resource to understand and visualize the event-level data included, it is not as useful as alternative formulations in modeling.\n",
        "* For the purposes of later work, the stacked versions of this data created in step two below are the version used.\n",
        "\n",
        "*   Previously, in the `06_generate_event_car_labels` script we created two dataframes containing labels corresponding to each event's CAR/SCAR value within a variety of different event window lengths, and subsequently wrote the output to a pickled dataframe:\n",
        "\n",
        "    * A df containing the results of applying two labels to the data by event_id\n",
        "    * A df containing the results of applying three labels to the data by event_id\n",
        "\n",
        "*   These dfs have identical structure, with naming convention for the label columns =\n",
        "`{car or scar}_{event window relative start}_{event window relative end}`\n",
        "\n",
        "\n",
        "* Rather than continuing to save this data as dataframes on Google Drive, **we will convert them to Datasets and push the Datasets to Hugging Face**\n",
        "\n",
        "* We will create two versions of the original data; the following step creates the first, a wide dataset with the following characteristics:\n",
        "\n",
        "    * num_rows = num unique event_id included in the data\n",
        "    * columns =\n",
        "        * event_id of each unique event in the dataset (essentially acts as a primary key)\n",
        "        * label data columns corresponding to each defined labeling permutation\n",
        "        * text columns containing two versions of each event's 8K text:\n",
        "            * the version of the 8K text that was cleaned and filtered in `07_obtain_and_clean_8k_text` immediately following its retreival\n",
        "            * the version of the 8k text following post processing using the function defined above (in order to format text as acceptable input to sec_base_shape)"
      ],
      "metadata": {
        "id": "mhUSbZESC2xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Relabel each label column in the 2 label and 3 label dataframes\n",
        "    to include the prefix '{2/3}_labels_'.\n",
        "This is necessary because these dataframes have identical column names,\n",
        "    and this allows us to distinguish between 2/3 label data in a\n",
        "    wide form that represents each event in a single row.\n",
        "'''\n",
        "\n",
        "col_rename_2_bins = {col:f'2_labels_{col}' for col\n",
        "                     in labels_2_bins.columns\n",
        "                     if col != 'event_id'}\n",
        "\n",
        "col_rename_3_bins = {col:f'3_labels_{col}' for col\n",
        "                     in labels_3_bins.columns\n",
        "                     if col != 'event_id'}\n",
        "\n",
        "labels_2_bins = labels_2_bins.rename(columns=col_rename_2_bins)\n",
        "labels_3_bins = labels_3_bins.rename(columns=col_rename_3_bins)\n",
        "\n",
        "'''\n",
        "Merge all the previously seperate dataframes into a single dataframe\n",
        "    using 'event_id' as the merge key.\n",
        "This creates our initial wide view, but does not yet contain the text\n",
        "    preprocessed using the function defined above.\n",
        "Truncate the text strings to be <= 1 million characters in order to\n",
        "    avoid issues with size limitations for spacy input (only 61 rws)\n",
        "'''\n",
        "\n",
        "labels = labels_2_bins.merge(labels_3_bins, how='left', on='event_id')\n",
        "\n",
        "text_w_labels_wide = text_cleaned.merge(labels,\n",
        "                                        how='left',\n",
        "                                        on='event_id').rename(\n",
        "                            columns={'text_8k': 'text_8k_sec_bert_base'})\n",
        "\n",
        "text_w_labels_wide['text_8k_sec_bert_base'] = text_w_labels_wide[\n",
        "    'text_8k_sec_bert_base'].str.slice(0,999999)"
      ],
      "metadata": {
        "id": "LeWXeZ4NoGCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now that we have created the wide dataset from our prior data, we make several adjustments before pushing the dataset to Hugging Face\n",
        "\n",
        "    1. **Convert each label column from its original integer format into ClassLabels**\n",
        "        * Includs logic required to set the number of classes and the names of each ClassLabel appropriately, as they differ for two and three level label cols\n",
        "    2. **Use the dataset map functionality to create the `text_8k_sec_bert_shape` data via the function defined above**\n",
        "        * This is the most efficient way to create this data\n",
        "    3. **Reorder the columns so that the order is more intuitive**\n",
        "        * Uses the select function and a list of the desired order\n",
        "\n",
        "**Once these final few steps are complete, the wide dataset is pushed to the Hub**\n",
        "\n"
      ],
      "metadata": {
        "id": "wYLyywFfeYia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_wide = Dataset.from_pandas(text_w_labels_wide,\n",
        "                                   split='train')\n",
        "\n",
        "new_features = dataset_wide.features.copy()\n",
        "for col_name in labels_2_bins.columns:\n",
        "\n",
        "    if col_name != 'event_id':\n",
        "\n",
        "        new_features[col_name] = ClassLabel(\n",
        "            num_classes = 2,\n",
        "            names=['non-neutral', 'neutral'])\n",
        "\n",
        "for col_name in labels_3_bins.columns:\n",
        "\n",
        "    if col_name != 'event_id':\n",
        "\n",
        "        new_features[col_name] = ClassLabel(\n",
        "            num_classes = 3,\n",
        "            names=['low', 'neutral', 'high'])\n",
        "\n",
        "dataset_wide = dataset_wide.cast(new_features)\n",
        "\n",
        "dataset_wide = dataset_wide.map(sec_bert_shape_preprocess, num_proc=8)\n",
        "\n",
        "ordered_col_list = [\n",
        "    'event_id','text_8k_sec_bert_base','text_8k_sec_bert_shape',\n",
        "    '2_labels_car_-5_5','2_labels_scar_-5_5','2_labels_car_-4_4',\n",
        "    '2_labels_scar_-4_4','2_labels_car_-3_3','2_labels_scar_-3_3',\n",
        "    '2_labels_car_-2_2','2_labels_scar_-2_2','2_labels_car_-1_1',\n",
        "    '2_labels_scar_-1_1','2_labels_car_-1_2','2_labels_scar_-1_2',\n",
        "    '2_labels_car_-1_3','2_labels_scar_-1_3','2_labels_car_-1_4',\n",
        "    '2_labels_scar_-1_4','2_labels_car_-1_5','2_labels_scar_-1_5',\n",
        "    '3_labels_car_-5_5','3_labels_scar_-5_5','3_labels_car_-4_4',\n",
        "    '3_labels_scar_-4_4','3_labels_car_-3_3','3_labels_scar_-3_3',\n",
        "    '3_labels_car_-2_2','3_labels_scar_-2_2','3_labels_car_-1_1',\n",
        "    '3_labels_scar_-1_1','3_labels_car_-1_2','3_labels_scar_-1_2',\n",
        "    '3_labels_car_-1_3','3_labels_scar_-1_3','3_labels_car_-1_4',\n",
        "    '3_labels_scar_-1_4','3_labels_car_-1_5','3_labels_scar_-1_5']\n",
        "\n",
        "dataset_wide = dataset_wide.select_columns(ordered_col_list)\n",
        "\n",
        "dataset_wide.push_to_hub('text_w_labels_wide')"
      ],
      "metadata": {
        "id": "nLYvzZKyn2uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create 2 Datasets (one to hold data for labels with 2 values, the other for the data for labels with 3 values)**\n",
        "\n",
        "* **NOTE: These are the datasets used in future work**\n",
        "\n",
        "\n",
        "*   This step essentially entails taking the values in what were components of the label column names in the wide dataset and converting them into descriptor values in new columns named\n",
        "    * `abnormal_return_metric` (scar or car)\n",
        "    * `event_window_start`\n",
        "    * `event_window_end`\n",
        "\n",
        "* The reason that label could not likewise be included and two datasets needed to be created is because we cannot stack two different ClassLabels (one with two values, one with three) in a single column.\n",
        "    * Although there were certainly workarounds (two label cols, with blank values in the column not corresponding to a given row subsets number of labels) they lead to less intuitive data structure, and were not pursued.\n",
        "\n",
        "* The end result of this process is the creation of two new stacked datasets where in each:\n",
        "\n",
        "    * num_rows = num unique event_id included in the data X the number of unique abnormal return metric values X the number of unique (event window start, event window end) tuples\n",
        "    * columns:\n",
        "        * event_id\n",
        "        * sec_text_8k_base\n",
        "        * sec_text_8k_shape\n",
        "        * abnormal_return_metric\n",
        "        * event_window_start\n",
        "        * event_window_end\n",
        "        * label\n",
        "\n",
        "* Following this reshaping, the data is pushed to the Hub as two datasets with identical shape:\n",
        "    * `dataset_stacked_two_labels`\n",
        "    * `dataset_stacked_three_labels`\n",
        "\n"
      ],
      "metadata": {
        "id": "7i-27UDxihj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_keep = ['event_id','text_8k_sec_bert_base','text_8k_sec_bert_shape']\n",
        "\n",
        "lst_2_labels_datasets_train = []\n",
        "lst_2_labels_datasets_test = []\n",
        "lst_3_labels_datasets_train = []\n",
        "lst_3_labels_datasets_test = []\n",
        "\n",
        "for col in dataset_wide.column_names:\n",
        "\n",
        "    if col not in to_keep:\n",
        "\n",
        "        cur_cols = to_keep + [col]\n",
        "\n",
        "        cur_dataset = dataset_wide.select_columns(cur_cols)\n",
        "        current_dataset = cur_dataset.rename_column(col, 'label')\n",
        "\n",
        "        col_split = col.split('_')\n",
        "\n",
        "        num_labels = [int(col_split[0])]*len(dataset_wide)\n",
        "        abnormal_return_metric = [col_split[-3]]*len(dataset_wide)\n",
        "        event_window_start = [int(col_split[-2])] * len(dataset_wide)\n",
        "        event_window_end = [int(col_split[-1])] * len(dataset_wide)\n",
        "\n",
        "        current_dataset = current_dataset.add_column(\n",
        "            'abnormal_return_metric', abnormal_return_metric)\n",
        "        current_dataset = current_dataset.add_column(\n",
        "            'event_window_start', event_window_start)\n",
        "        current_dataset = current_dataset.add_column(\n",
        "            'event_window_end', event_window_end)\n",
        "\n",
        "        if num_labels[0] == 2:\n",
        "            current_dataset = current_dataset.train_test_split(\n",
        "                test_size = .2,\n",
        "                stratify_by_column = 'label')\n",
        "\n",
        "            lst_2_labels_datasets_train.append(current_dataset['train'])\n",
        "            lst_2_labels_datasets_test.append(current_dataset['test'])\n",
        "\n",
        "        elif num_labels[0] == 3:\n",
        "            current_dataset = current_dataset.train_test_split(\n",
        "                test_size = .2,\n",
        "                stratify_by_column = 'label')\n",
        "\n",
        "            lst_3_labels_datasets_train.append(current_dataset['train'])\n",
        "            lst_3_labels_datasets_test.append(current_dataset['test'])"
      ],
      "metadata": {
        "id": "UmAcT-zMnCMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_stacked_2_labels_train = concatenate_datasets(\n",
        "    lst_2_labels_datasets_train)\n",
        "dataset_stacked_2_labels_test = concatenate_datasets(\n",
        "    lst_2_labels_datasets_test)\n",
        "\n",
        "dataset_stacked_2_labels = DatasetDict({\n",
        "    'train': dataset_stacked_2_labels_train,\n",
        "    'test': dataset_stacked_2_labels_test\n",
        "})\n",
        "\n",
        "dataset_stacked_3_labels_train = concatenate_datasets(\n",
        "    lst_3_labels_datasets_train)\n",
        "dataset_stacked_3_labels_test = concatenate_datasets(\n",
        "    lst_3_labels_datasets_test)\n",
        "\n",
        "dataset_stacked_3_labels = DatasetDict({\n",
        "    'train': dataset_stacked_3_labels_train,\n",
        "    'test': dataset_stacked_3_labels_test\n",
        "})\n",
        "\n",
        "dataset_stacked_2_labels.push_to_hub('dataset_stacked_2_labels')\n",
        "dataset_stacked_3_labels.push_to_hub('dataset_stacked_3_labels')"
      ],
      "metadata": {
        "id": "QA8etqcpr2ty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
